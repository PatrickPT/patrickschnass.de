<!doctype html>
<html
  lang="de-DE"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="false"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="de-DE">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title>Latent Diffusion Models: What is all the fuzz about? &middot; Patrick Schnaß | AI Advisory</title>
    <meta name="title" content="Latent Diffusion Models: What is all the fuzz about? &middot; Patrick Schnaß | AI Advisory">
  

  
  
    <meta name="description" content="Learn how Latent Diffusion Models work and the mathematical intuition behind them.">
  
  
    <meta name="keywords" content="Math,stable-diffusion,latent-diffusion-models,Notes,">
  
  
  
  <link rel="canonical" href="http://localhost:1313/posts/latent-diffusion-models/">
  

  
  
    <meta name="author" content="Patrick Schnaß">
  
  
    
      
        
          <link href="https://www.linkedin.com/in/patrickschnass/" rel="me">
        
      
    
      
        
          <link href="https://github.com/PatrickPT" rel="me">
        
      
    
  

  
  <meta property="og:url" content="http://localhost:1313/posts/latent-diffusion-models/">
  <meta property="og:site_name" content="Patrick Schnaß | AI Advisory">
  <meta property="og:title" content="Latent Diffusion Models: What is all the fuzz about?">
  <meta property="og:description" content="Learn how Latent Diffusion Models work and the mathematical intuition behind them.">
  <meta property="og:locale" content="de_DE">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-01-20T14:50:18+01:00">
    <meta property="article:modified_time" content="2023-01-20T14:50:18+01:00">
    <meta property="article:tag" content="Math">
    <meta property="article:tag" content="Stable-Diffusion">
    <meta property="article:tag" content="Latent-Diffusion-Models">
    <meta property="article:tag" content="Notes">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Latent Diffusion Models: What is all the fuzz about?">
  <meta name="twitter:description" content="Learn how Latent Diffusion Models work and the mathematical intuition behind them.">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
    
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.9b97f58406c0ad04d41a74bbc9d276dd79c12e1333bd83f8b1708995b0f2cacb235e5f35a2e51463f3fd138c5fdab0213eb051a5590dc9bc61bc8998fa56464b.css"
    integrity="sha512-m5f1hAbArQTUGnS7ydJ23XnBLhMzvYP4sXCJlbDyyssjXl81ouUUY/P9E4xf2rAhPrBRpVkNybxhvImY&#43;lZGSw==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.eb1cab4fe4b9f760444cfbaf02c7410770218e3a2789ec108c0d7d741aa0fbc02e43567bc4e4f2ed05e6acc6096e291a4ff0cf8e6e852fcc31e4be9e48c33c63.js"
      integrity="sha512-6xyrT&#43;S592BETPuvAsdBB3AhjjoniewQjA19dBqg&#43;8AuQ1Z7xOTy7QXmrMYJbikaT/DPjm6FL8wx5L6eSMM8Yw=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  








  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "Latent Diffusion Models: What is all the fuzz about?",
    "headline": "Latent Diffusion Models: What is all the fuzz about?",
    
    "inLanguage": "de-DE",
    "url" : "http://localhost:1313/posts/latent-diffusion-models/",
    "author" : {
      "@type": "Person",
      "name": "Patrick Schnaß"
    },
    "copyrightYear": "2023",
    "dateCreated": "2023-01-20T14:50:18\u002b01:00",
    "datePublished": "2023-01-20T14:50:18\u002b01:00",
    
    "dateModified": "2023-01-20T14:50:18\u002b01:00",
    
    "keywords": ["Math","stable-diffusion","latent-diffusion-models","Notes"],
    
    "mainEntityOfPage": "true",
    "wordCount": "3481"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral bf-scrollbar">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 z-100">
  <div
    id="menu-blur"
    class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl bg-neutral/25 dark:bg-neutral-800/25"></div>
  <div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32">
    <div class="main-menu flex items-center w-full gap-2 p-1 pl-0">
  
    
    
      <div>
        <a href="/" class="flex">
          <span class="sr-only">Patrick Schnaß | AI Advisory</span>
          
            <img
              src="/favicon/android-chrome-64x64.png"
              width="25"
              height="30"
              class="logo max-h-20 max-w-20 object-scale-down object-left nozoom"
              alt="">
          
        </a>
      </div>
    
  
  
    <a href="/" class="text-base font-medium truncate min-w-0 shrink">
      Patrick Schnaß | AI Advisory
    </a>
  
  <div class="flex items-center ms-auto">
    <div class="hidden md:flex">
      <nav class="flex items-center gap-x-5 h-12">
  
    
      
  
    <a
      href="/"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="Home"
      title="">
      
      
        <span class="text-base font-medium break-normal">
          Home
        </span>
      
    </a>
  

    
      
  
    <a
      href="/#contact"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="Contact"
      title="">
      
      
        <span class="text-base font-medium break-normal">
          Contact
        </span>
      
    </a>
  

    
      
  
    <a
      href="/#services"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="Services"
      title="">
      
      
        <span class="text-base font-medium break-normal">
          Services
        </span>
      
    </a>
  

    
      
  
    <a
      href="/about"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="About"
      title="">
      
      
        <span class="text-base font-medium break-normal">
          About
        </span>
      
    </a>
  

    
      
  
    <a
      href="/posts"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="Blog"
      title="">
      
      
        <span class="text-base font-medium break-normal">
          Blog
        </span>
      
    </a>
  

    
      
  
    <a
      href="https://www.linkedin.com/in/patrickschnass/"
      
        target="_blank"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="linkedin"
      title="">
      
        <span >
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span>
        </span>
      
      
    </a>
  

    
      
  
    <a
      href="https://github.com/PatrickPT"
      
        target="_blank"
      
      class="flex items-center bf-icon-color-hover"
      aria-label="github"
      title="">
      
        <span >
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span>
        </span>
      
      
    </a>
  

    
  

  

  

  

  
    <div class="flex items-center">
      <button
        id="appearance-switcher"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base bf-icon-color-hover">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    </div>
  
</nav>



    </div>
    <div class="flex md:hidden">
      <div class="flex items-center h-14 gap-4">
  

  
    <button
      id="appearance-switcher-mobile"
      type="button"
      aria-label="Dark mode switcher"
      class="flex items-center justify-center text-neutral-900 hover:text-primary-600 dark:text-neutral-200 dark:hover:text-primary-400">
      <div class="dark:hidden">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
      </div>
      <div class="hidden dark:block">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
      </div>
    </button>
  

  
    <input type="checkbox" id="mobile-menu-toggle" autocomplete="off" class="hidden peer">
    <label for="mobile-menu-toggle" class="flex items-center justify-center cursor-pointer bf-icon-color-hover">
      <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
    </label>

    <div
      role="dialog"
      aria-modal="true"
      style="scrollbar-gutter: stable;"
      class="fixed inset-0 z-50 invisible overflow-y-auto px-6 py-20 opacity-0 transition-[opacity,visibility] duration-300 peer-checked:visible peer-checked:opacity-100 bg-neutral-50/97 dark:bg-neutral-900/99
      bf-scrollbar">
      <label
        for="mobile-menu-toggle"
        class="fixed end-8 top-5 flex items-center justify-center z-50 h-12 w-12 cursor-pointer select-none rounded-full bf-icon-color-hover border bf-border-color bf-border-color-hover bg-neutral-50 dark:bg-neutral-900">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </label>
      <nav class="mx-auto max-w-md space-y-6">
        
  
    
    <div class="px-2">
      <a
        href="/"
        aria-label="Home"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
        <span title="" class="text-2xl font-bold tracking-tight">
          Home
        </span>
        
      </a>

      
    </div>
  
    
    <div class="px-2">
      <a
        href="/#contact"
        aria-label="Contact"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
        <span title="" class="text-2xl font-bold tracking-tight">
          Contact
        </span>
        
      </a>

      
    </div>
  
    
    <div class="px-2">
      <a
        href="/#services"
        aria-label="Services"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
        <span title="" class="text-2xl font-bold tracking-tight">
          Services
        </span>
        
      </a>

      
    </div>
  
    
    <div class="px-2">
      <a
        href="/about"
        aria-label="About"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
        <span title="" class="text-2xl font-bold tracking-tight">
          About
        </span>
        
      </a>

      
    </div>
  
    
    <div class="px-2">
      <a
        href="/posts"
        aria-label="Blog"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
        <span title="" class="text-2xl font-bold tracking-tight">
          Blog
        </span>
        
      </a>

      
    </div>
  
    
    <div class="px-2">
      <a
        href="https://www.linkedin.com/in/patrickschnass/"
        aria-label="linkedin"
        
          target="_blank"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
          <span class="flex items-center justify-center h-8 w-8 text-2xl">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span>
          </span>
        
        <span title="" class="text-2xl font-bold tracking-tight">
          
        </span>
        
      </a>

      
    </div>
  
    
    <div class="px-2">
      <a
        href="https://github.com/PatrickPT"
        aria-label="github"
        
          target="_blank"
        
        class="flex items-center gap-4 group bf-icon-color-hover text-neutral-700 dark:text-neutral-200">
        
          <span class="flex items-center justify-center h-8 w-8 text-2xl">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span>
          </span>
        
        <span title="" class="text-2xl font-bold tracking-tight">
          
        </span>
        
      </a>

      
    </div>
  

        
  

        
  

      </nav>
    </div>
  
</div>







    </div>
  </div>
</div>





  </div>
</div>


<script
  type="text/javascript"
  src="/js/background-blur.min.605b3b942818f0ab5a717ae446135ec46b8ee5a2ad12ae56fb90dc2a76ce30c388f9fec8bcc18db15bd47e3fa8a09d779fa12aa9c184cf614a315bc72c6c163d.js"
  integrity="sha512-YFs7lCgY8KtacXrkRhNexGuO5aKtEq5W&#43;5DcKnbOMMOI&#43;f7IvMGNsVvUfj&#43;ooJ13n6EqqcGEz2FKMVvHLGwWPQ=="
  data-blur-id="menu-blur"></script>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        Latent Diffusion Models: What is all the fuzz about?
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  
    
  

  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2023-01-20T14:50:18&#43;01:00">January 20, 2023</time><span class="px-2 text-primary-500">&middot;</span><span>3481 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">17 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
    
    
      
    
    
      
      
      
        
        
        
      
      <img
        class="!mt-0 !mb-0 h-24 w-24 rounded-full me-4"
        width="96"
        height="96"
        alt="Patrick Schnaß"
        src="/patrick_hu_fe0c2285bbac30b1.png"
        data-zoom-src="/patrick_hu_a5a586b022a9f652.png">
    
  
  <div class="place-self-center">
    
      <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
        Author
      </div>
      <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
        Patrick Schnaß
      </div>
    
    
      <div class="text-sm text-neutral-700 dark:text-neutral-400">Helping enterprises turn AI initiatives into profitable business processes</div>
    
    <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://www.linkedin.com/in/patrickschnass/"
          target="_blank"
          aria-label="Linkedin"
          title="Linkedin"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></span></a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/PatrickPT"
          target="_blank"
          aria-label="Github"
          title="Github"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a
        >
      
    
  </div>

</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h1 class="relative group">TL;DR
    <div id="tldr" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#tldr" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p>The following learning notes try to give some intuition on how <a href="https://stability.ai/blog/stable-diffusion-public-release"  target="_blank">Stable Diffusion</a> works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.</p>
<p><strong>The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.</strong></p>
<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/"  target="_blank">Illustrations by Jay Alammar</a></p>
<p><a href="https://arxiv.org/abs/1503.03585"  target="_blank">Sohl-Dickstein et al., 2015</a></p>
<p><a href="https://arxiv.org/abs/2006.11239"  target="_blank">Ho et al. 2020</a></p>
<p><a href="https://arxiv.org/abs/2112.10752"  target="_blank">Rombach &amp; Blattmann, et al. 2022</a></p>
<p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"  target="_blank">Lilian Weng on Diffusion Models</a></p>
<p><a href="https://theaisummer.com/diffusion-models/"  target="_blank">Sergios Karagiannakos on Diffusion Models</a></p>
<p><a href="https://huggingface.co/blog/annotated-diffusion"  target="_blank">Hugging Face on Annotated Diffusion Model</a></p>
<p>If you recently introduced yourself to strangers and told them about your job in AI, there is a good chance they asked you about the current hype-train around the next generation of generative AI models and related data products like Dall-E, Stable Diffusion or Midjourney.</p>
<p>Latent Diffusion Models like the ones above had some significant media attention. While no one outside AI community bats an eye if Deepmind creates an algorithm, that beats the (almost ancient) and important Strassen-Algorithm by some percent in computation complexity(which is a tremendous progress), nearly everyone is excited to create made up pictures of cats doing crazy stuff through a simple interface.</p>
<p><a href="/posts/2023_01_11_latent_diffusion_models/images/cat_surfing.jpeg" ><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="cat-surfing"
    src="/posts/2023_01_11_latent_diffusion_models/images/cat_surfing.jpeg"
    ></figure>
</a>
<em>Stable Diffusion &ldquo;cat surfing waves at sunset, comic style&rdquo;</em></p>
<p>While those models already made a name for themselves by winning <a href="https://news.artnet.com/art-world/colorado-artists-mad-ai-art-competition-2168495"  target="_blank">art competitions</a>, are adopted by companies into their related data products(Canva.com, Shutterstock.com) and start-ups creating those products raising billions in <a href="https://www.bloomberg.com/news/articles/2022-10-17/digital-media-firm-stability-ai-raises-funds-at-1-billion-value"  target="_blank">venture capital</a> you may ask yourself:</p>
<ul>
<li>What is all the fuzz about? </li>
<li>What is behind the hype? What are Latent Diffusion Models? </li>
<li>What is the math behind them? </li>
<li>Do they impact my life? What is the best way to leverage their power?  </li>
</ul>
<p>Let me briefly introduce you to Diffusion Models and Latent Diffusion Models and explain the math.</p>
<p>If you are interested in a Hands-On you can find that in my other post:</p>
<p><a href="/posts/hands-on-latent-diffusion-models" >Hands on Latent Diffusion Models</a></p>
<p>If you want an awesome visual introduction with diagrams i strongly advise to visit the <a href="https://jalammar.github.io/illustrated-stable-diffusion/"  target="_blank">blog by Jay Alammar</a>.</p>

<h1 class="relative group">What is this post about:
    <div id="what-is-this-post-about" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#what-is-this-post-about" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p>A brand-new category of cutting-edge generative models called diffusion models generate a variety of high-resolution images.
They have already received a great deal of attention as a result of OpenAI, Nvidia, and Google&rsquo;s success in training massive models.
We&rsquo;ll take a deeper look into <strong>Denoising Diffusion Probabilistic Models</strong> (also known as DDPMs, diffusion models, score-based generative models or simply <a href="https://benanne.github.io/2022/01/31/diffusion.html"  target="_blank">autoencoders</a>) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include <a href="https://arxiv.org/abs/2112.10741"  target="_blank">GLIDE</a> and <a href="https://openai.com/dall-e-2/"  target="_blank">DALL-E 2</a> by OpenAI, <a href="https://github.com/CompVis/latent-diffusion"  target="_blank">Latent Diffusion</a> by the University of Heidelberg and <a href="https://imagen.research.google/"  target="_blank">ImageGen</a> by Google Brain.</p>

<h1 class="relative group">Intuition on Diffusion Models
    <div id="intuition-on-diffusion-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#intuition-on-diffusion-models" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p>You may ask &ldquo;What is the intuition behind diffusion models?&rdquo;
Let&rsquo;s break it down with a short example to make it clear:
You are a painter hired by Vatican with the task to repaint the fresco at the ceiling of <a href="https://de.wikipedia.org/wiki/Sixtinische_Kapelle#/media/Datei:CAPPELLA_SISTINA_Ceiling.jpg"  target="_blank">sixtinian chapel</a>.</p>
<p>The requirement is to recreate the fresco with pictures of cats. The requirement is based on the old fresco and the vatican wants to have the same scenes as currently there but witch cats.
So you start remembering the fresco and start bringing up a base coat and the old fresco soon becomes a big grey noise.</p>
<p>Most likely you will be overwhelmed with creating a big fresco and immediately think of structuring your work into smaller chunks.
Then you may outline the structures you want to paint based on your memory on the old picture and your vision on the new one.</p>
<p>Maybe you start with one object like the arm of Adam, from <a href="https://en.wikipedia.org/wiki/The_Creation_of_Adam"  target="_blank">the creation of Adam</a> and then focus on the hand. Gradually you add more details and finally are happy with the scene, decide to finish it and tackle the next part. Still later you may change things after you decided that it fits better with the overall fresco.</p>
<p>Finally you are building a masterpiece lasting centuries until someone thinks dogs are nicer than cats(so never).</p>
<p>The same idea comes with Diffusion: Gradually add noise and create the best representation of the input vision in many small steps. Breaking up the image sampling allows the models to correct itself over those small steps iteratively and produces a good sample.</p>
<p>Unfortunately nothing is cost-neutral. Like it will cost a painter like Michealangelo almost 4 years to finish the fresco in Sixtinian chapel, the iterative process makes the Models slow at sampling(At least compared to GANs).</p>

<h1 class="relative group">What are Diffusion Models?
    <div id="what-are-diffusion-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#what-are-diffusion-models" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p>The idea of diffusion for generative modeling was introduced in (<a href="https://arxiv.org/abs/1503.03585"  target="_blank">Sohl-Dickstein et al., 2015</a>). However, it took until (<a href="https://arxiv.org/abs/1907.05600"  target="_blank">Song et al., 2019</a>) (at Stanford University), and then (<a href="https://arxiv.org/abs/2006.11239"  target="_blank">Ho et al., 2020</a>) (at Google Brain) who independently improved the approach.
DDPM  which we are focussing on originally was introduced in a paper by (<a href="https://arxiv.org/abs/2006.11239"  target="_blank">Ho et al., 2020</a>).</p>
<p>At a high level, they work by first representing the input signal as a set of latent variables, which are then transformed through a series of probabilistic transformations to produce the output signal. The transformation process is designed to smooth out the noise in the input signal and reconstruct a cleaner version of the signal.</p>
<p>Transformation consist of 2 processes.</p>
<p>In a bit more detail for images, the set-up consists of 2 processes:</p>
<ul>
<li>a fixed (or predefined) forward diffusion process \(q\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise</li>
<li>a learned reverse denoising diffusion process \(p_\theta\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.</li>
</ul>
<p>Diffusion Models are basically generative models:
<a href="/posts/2023_01_11_latent_diffusion_models/images/types_gans.png" ><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="types_gans"
    src="/posts/2023_01_11_latent_diffusion_models/images/types_gans.png"
    ></figure>
</a>
<em>Overview of the different types of generative <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/HowdoestheDiffusionprocesswork?"  target="_blank">models</a></em></p>

<h1 class="relative group">I want to see the math
    <div id="i-want-to-see-the-math" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#i-want-to-see-the-math" aria-label="Anchor">#</a>
    </span>
    
</h1>

<h2 class="relative group">Diffusion
    <div id="diffusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#diffusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<!-- At a high level, Diffusion Models work by first representing the input signal as a set of latent variables, which are then transformed through a series of probabilistic transformations to produce the output signal. The transformation process is designed to smooth out the noise in the input signal and reconstruct a cleaner version of the signal. --> 
<p>In probability theory and statistics, diffusion processes are a class of continuous-time <a href="https://en.wikipedia.org/wiki/Markov_chain"  target="_blank">Markov</a> process with almost surely continuous sample paths. E.g. <a href="https://en.wikipedia.org/wiki/Brownian_motion"  target="_blank">Brownian motion</a></p>
<p>Wikipedia says:</p>
<ul>
<li>
<p>A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</p>
</li>
<li>
<p>A continuous-time Markov chain (CTMC) is a continuous stochastic process in which, for each state, the process will change state according to an exponential random variable and then move to a different state as specified by the probabilities of a stochastic matrix</p>
</li>
</ul>
<p>Diffusion consists of 2 processes:</p>
<ul>
<li>a fixed (or predefined) forward diffusion process $q$ of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise</li>
<li>a learned reverse denoising diffusion process $p_\theta$, where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.</li>
</ul>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="DDPM"
    src="/posts/2023_01_11_latent_diffusion_models/images/DDPM_pres.png"
    ></figure>

<em>The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: Ho et al. 2020) and <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"  target="_blank">Lilian Weng</a></em></p>

<h2 class="relative group">Forward Diffusion
    <div id="forward-diffusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#forward-diffusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Given a data point from a real data distribution $x_0 \sim q(x)$ we define a
<strong>forward diffusion</strong> in which we add small Gaussian noise stepwise for $T$ steps
producing noisy samples $\mathbf{x}_1, \dots, \mathbf{x}_T$</p>
<p>Step sizes are controlled by a variance schedule $0 &lt; \beta_1 &lt; \beta_2 &lt; &hellip; &lt; \beta_T &lt; 1$.</p>
<p>It is defined as</p>
<p>$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t \mathbf{I})$
with
$\sqrt{1 - \beta_t} x_{t-1}$ as decay towards origin and</p>
<p>$\beta_t \mathbf{I}$ as the addition of small noise.</p>
<p>Rephrased:
A normal distribution (also called Gaussian distribution) is defined by 2 parameters:</p>
<ul>
<li>a mean $\mu$ and</li>
<li>a variance $\sigma^2 \geq 0$.</li>
</ul>
<p>Basically, each new (slightly noisier) image at time step $t$ is drawn from a <strong>conditional Gaussian distribution</strong> with</p>
<ul>
<li>$\mathbf{\mu}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}$ and</li>
<li>$\sigma^2_t = \beta_t$,</li>
</ul>
<p>which we can do by sampling $\mathbf{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ and then setting $\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} +  \sqrt{\beta_t} \mathbf{\epsilon}$.</p>
<p>Given a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, you end up with what is called an <a href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic"  target="_blank">isotropic Gaussian distribution</a> at $t=T$ via a gradual process.</p>
<p>Isotropic means the probability density is equal (iso) in every direction (tropic). In gaussians this can be achieved with a $\sigma^2 I$ covariance matrix.</p>
<p>One property of the diffusion process is, that you can sample $x_t$ at any time step $t$ using <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick"  target="_blank">reparameterization trick</a>.
Let $\alpha_{t} = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$</p>
<p>$$
\begin{aligned}
\mathbf{x}_t
&amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} &amp; \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} &amp; \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} &amp; \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians.} \\
&amp;= \dots \\
&amp;= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}\\
q(\mathbf{x}_t \vert \mathbf{x}_0)
&amp;= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$</p>
<p>So the sampling of noise and creation of $x_t$ is done in one step only and can be sampled at any timestep.</p>

<h2 class="relative group">Reverse Diffusion
    <div id="reverse-diffusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#reverse-diffusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>$q(x_{t-1} \vert x_t)$ which denotes the Reverse Process is intractable since statistical estimates of it require computations involving the entire dataset and therefore we need to learn a model $p_0$ to approximate these conditional probabilities in order to run the reverse diffusion process.</p>
<p>We need to learn a model $p_0$ to approximate these conditional probabilities</p>
<p>Since $q(x_{t-1} \vert x_t)$ will also be Gaussian, for small enough $\beta_t$, we can choose $p_0$ to be Gaussian and just parameterize the mean and variance as the <strong>Reverse Diffusion</strong>:</p>
<p>$\quad p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N} (\mathbf{x}_{t-1}; {\mu}_\theta(\mathbf{x}_t, t), {\Sigma}_\theta(\mathbf{x}_t, t))
$</p>
<p>with
$\mu_\theta(x_{t},t)$ as the mean and
$\Sigma_\theta (x_{t},t)$ as the variance</p>
<p>conditioned on the noise level $t$ as the to be learned functions of drift and covariance of the Gaussians(by a Neural Net).</p>
<p>As the target image is already defined the problem can be described as a supervised learning problem.</p>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/diffusion-example_pres.png"
    ></figure>
</p>
<p><em>An example of training a diffusion model for modeling a 2D swiss roll data. (Image source: Sohl-Dickstein et al., 2015)</em></p>
<p>Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to <strong>keep the variance fixed, and let the neural network only learn (represent) the mean $\mu_\theta$ of this conditional probability distribution</strong>.</p>

<h2 class="relative group">Optimization of the Loss Function
    <div id="optimization-of-the-loss-function" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#optimization-of-the-loss-function" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To derive an objective function to learn the mean of the backward process, the authors observe that the combination of $q$ and $p_\theta$ can be seen as a variational auto-encoder (VAE) <a href="https://arxiv.org/abs/1312.6114"  target="_blank">(Kingma et al., 2013)</a>.</p>
<p>A Diffusion Model can be trained by finding the reverse Markov transitions that maximize the likelihood of the training data. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood</p>
<p>$- \log p_\theta(\mathbf{x}_0)$.</p>
<p>After transformation(<a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"  target="_blank">Lilian Weng</a> for reference), we can write the evidence lower bound (ELBO) as follows:</p>
<p>$$
\begin{aligned}
- \log p_\theta(\mathbf{x}_0)
&amp;\leq - \log p_\theta(\mathbf{x}_0)+ D_\text{KL}(q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) \vert p_\theta(\mathbf{x}_{1:T}\vert\mathbf{x}_0) )
\end{aligned}
$$</p>
<p>Intuition on the optimization:
For a function $f(x)$, which can&rsquo;t be computed(like e.g. the above negative log-likelihood) and have also a function $g(x)$, which we can compute and fullfills the condition $g(x) &lt;= f(x)$. If we then maximize $g(x)$ we can be certain that $f(x)$ will also increase.</p>
<p>For optimization we use <strong>Kullback-Leibler (KL) Divergences</strong>.
The KL Divergence is a statistical distance measure of how much one probability distribution $P$ differs from a reference distribution $Q$.</p>
<p>We are interested in formulating the Loss function in terms of KL divergences because the transition distributions in our Markov chain are Gaussians, and the KL divergence between Gaussians has a closed form.
For a closer look please look <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/"  target="_blank">here</a></p>
<p>If we rewrite the above Loss function and apply the bayesian rule the upper term can be summarized to a joint probability and will be trainsformed to the Variational Lower Bound:</p>
<p>$$
\begin{aligned}
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}) / p_\theta(\mathbf{x}_0)} \Big] \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&amp;= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
\text{Let }L_\text{VLB}
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \geq - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0)
\end{aligned}
$$</p>
<p>Complete calculation can be found <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"  target="_blank">here</a> together with a really nice explanation <a href="https://www.youtube.com/watch?v=HoKDTa5jHvg&amp;t=905s"  target="_blank">here</a></p>
<p>The objective can be further rewritten to be a combination of several KL-divergence and entropy terms(Detailed process in Appendix B in <a href="https://arxiv.org/abs/1503.03585"  target="_blank">Sohl-Dickstein et al., 2015</a>)</p>
<p>$$
\begin{aligned}
L_\text{VLB}
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&amp;= \dots \\
&amp;= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]
\end{aligned}
$$</p>
<p>Reshaped:</p>
<p>$$
\begin{aligned}
L_\text{VLB} &amp;= L_T + L_{T-1} + \dots + L_0 \\
\text{where } L_T &amp;= D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \\
L_t &amp;= D_\text{KL}(q(\mathbf{x}_t \vert \mathbf{x}_{t+1}, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_t \vert\mathbf{x}_{t+1})) \text{ for }1 \leq t \leq T-1 \\
L_0 &amp;= - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)
\end{aligned}
$$</p>
<p>Every KL term in $L_\text{VLB}$ except for $L_0$ compares two Gaussian distributions and therefore they can be computed in closed form. $L_T$ is constant and can be ignored during training because $q$ has no learnable parameters and $x_T$ is a Gaussian noise. $L_t$ formulates the difference between the desired denoising steps and the approximated ones.</p>
<p>It is evident that through the ELBO, maximizing the likelihood boils down to learning the denoising steps $L_t$.</p>
<p>We would like to train $\boldsymbol{\mu}_\theta$ to predict $\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)$.
Because $\mathbf{x}_t$ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict $\boldsymbol{\epsilon}_t$ from the input  $\mathbf{x}_t$ at time step $t$:</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &amp;= \color{red}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \\
\text{Thus }\mathbf{x}_{t-1} &amp;= \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\end{aligned}
$$</p>
<p>The loss term $L_t$ is parameterized to minimize the difference from $\tilde\mu$ :</p>
<p>$$
\begin{aligned}
L_t
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2 | \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) |^2_2} | \color{blue}{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)} - \color{green}{\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)} |^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2  |\boldsymbol{\Sigma}_\theta |^2_2} | \color{blue}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)} - \color{green}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) \Big)} |^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) | \boldsymbol{\Sigma}_\theta |^2_2} |\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)|^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) | \boldsymbol{\Sigma}_\theta |^2_2} |\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)|^2 \Big]
\end{aligned}
$$</p>
<p>The final objective function $L_t$ then looks as follows (for a random time step $t$ given $\mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ ) <a href="https://arxiv.org/abs/2006.11239"  target="_blank">as shown by Ho et al. (2020)</a></p>
<p>$$ | \mathbf{\epsilon} - \mathbf{\epsilon}_\theta(\mathbf{x}_t, t) |^2 = | \mathbf{\epsilon} - \mathbf{\epsilon}_\theta( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\epsilon}, t) |^2.$$</p>
<p>Here, $\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\mathbf{\epsilon}$ is the pure noise sampled at time step $t$, and $\mathbf{\epsilon}_\theta (\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.</p>
<p>Here, $\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\mathbf{\epsilon}$ is the pure noise sampled at time step $t$, and $\mathbf{\epsilon}_\theta (\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.</p>
<p>The training algorithm now looks as follows:</p>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/training_pres.png"
    ></figure>
</p>
<p>In other words:</p>
<ul>
<li>we take a random sample $\mathbf{x}_0$ from the real unknown and possibily complex data distribution $q(\mathbf{x}_0)$</li>
<li>we sample a noise level $t$ uniformally between $1$ and $T$ (i.e., a random time step)</li>
<li>we sample some noise from a Gaussian distribution and corrupt the input by this noise at level $t$ (using the nice property defined above)</li>
<li>the neural network is trained to predict this noise based on the corrupted image $\mathbf{x}_t$ (i.e. noise applied on $\mathbf{x}_0$ based on known schedule $\beta_t$ )</li>
</ul>

<h1 class="relative group">Neural Nets
    <div id="neural-nets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#neural-nets" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p>The neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this?</p>
<p>What is typically used here is very similar to that of an Autoencoder, which you may remember from typical &ldquo;intro to deep learning&rdquo; tutorials. Autoencoders have a so-called &ldquo;bottleneck&rdquo; layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the &ldquo;bottleneck&rdquo;, and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.</p>
<p>In terms of architecture, the DDPM authors went for a U-Net, introduced by (Ronneberger et al., 2015) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in He et al., 2015).</p>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/unet_architecture.jpg"
    ></figure>
</p>

<h1 class="relative group">The math on Latent Diffusion
    <div id="the-math-on-latent-diffusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#the-math-on-latent-diffusion" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p>It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process, as
can be up to one or a few thousand steps. One data point from Song et al. 2020: “For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM, but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU.”</p>
<p>Latent diffusion model (LDM; Rombach &amp; Blattmann, et al. 2022) runs the diffusion process in the latent space instead of pixel space, making training cost lower and inference speed faster. It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.</p>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/rombach-latent-space.png"
    ></figure>
</p>
<p>It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression.</p>
<p>LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.</p>
<p>The perceptual compression process relies on an autoencoder model.</p>
<p>An encoder $\mathcal{E}$ is used to compress the input image $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$ to a smaller 2D latent vector $\mathbf{z} = \mathcal{E}(\mathbf{x}) \in \mathbb{R}^{h \times w \times c}$ , where the downsampling rate $f=H/h=W/w=2^m, m \in \mathbb{N}$.</p>
<p>Then an decoder $\mathcal{D}$ reconstructs the images from the latent vector, $\tilde{\mathbf{x}} = \mathcal{D}(\mathbf{z})$.</p>
<p>The paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces.</p>
<ul>
<li>KL-reg: A small KL penalty towards a standard normal distribution over the learned latent, similar to <a href="https://lilianweng.github.io/posts/2018-08-12-vae/"  target="_blank">VAE</a>.</li>
<li>VQ-reg: Uses a vector quantization layer within the decoder, like <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#vq-vae-and-vq-vae-2"  target="_blank">VQVAE</a> but the quantization layer is absorbed by the decoder.</li>
</ul>
<p>The diffusion and denoising processes happen on the latent vector $\mathbf{z}$. The denoising model is a time-conditioned U-Net, augmented with the cross-attention mechanism to handle flexible conditioning information for image generation (e.g. class labels, semantic maps, blurred variants of an image).</p>
<p>The design is equivalent to fuse representation of different modality into the model with cross-attention mechanism.</p>
<p>Each type of conditioning information is paired with a domain-specific encoder $\tau_\theta$ to project the conditioning input $y$ to an intermediate representation that can be mapped into cross-attention component, $\tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}$:</p>
<p>$$
\begin{aligned}
&amp;\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\Big(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\Big) \cdot \mathbf{V} \\
&amp;\text{where }\mathbf{Q} = \mathbf{W}^{(i)}_Q \cdot \varphi_i(\mathbf{z}_i),; \mathbf{K} = \mathbf{W}^{(i)}_K \cdot \tau_\theta(y),; \mathbf{V} = \mathbf{W}^{(i)}_V \cdot \tau_\theta(y) \\
&amp;\text{and } \mathbf{W}^{(i)}_Q \in \mathbb{R}^{d \times d^i_\epsilon},; \mathbf{W}^{(i)}_K, \mathbf{W}^{(i)}_V \in \mathbb{R}^{d \times d_\tau},; \varphi_i(\mathbf{z}_i) \in \mathbb{R}^{N \times d^i_\epsilon},; \tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}
\end{aligned}
$$</p>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/rombach-latent-space-comments.jpg"
    ></figure>

<em>Picture from <a href="https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46"  target="_blank">J. Rafid Siddiqui</a></em></p>

<h1 class="relative group">And what is the result?
    <div id="and-what-is-the-result" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#and-what-is-the-result" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/cat_math.jpeg"
    ></figure>

<em>Stable Diffusion &ldquo;a cat looking at the ocean at sunset&rdquo;</em></p>
<p><figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt=""
    src="/posts/2023_01_11_latent_diffusion_models/images/cat_math2.jpeg"
    ></figure>

<em>Stable Diffusion &ldquo;a cat looking at the ocean at sunset&rdquo;</em></p>

<h1 class="relative group">Ressources
    <div id="ressources" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ressources" aria-label="Anchor">#</a>
    </span>
    
</h1>
<p><a href="https://huggingface.co/blog/annotated-diffusion"  target="_blank">The Annotated Diffusion Model</a></p>
<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/"  target="_blank">Illustrations by Jay Alammar</a></p>
<p><a href="https://arxiv.org/abs/1503.03585"  target="_blank">Sohl-Dickstein et al., 2015</a></p>
<p><a href="https://arxiv.org/abs/2112.10752"  target="_blank">Rombach &amp; Blattmann, et al. 2022</a></p>
<p><a href="https://arxiv.org/abs/2006.11239"  target="_blank">Ho et al. 2020</a></p>
<p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"  target="_blank">Lilian Weng on Diffusion Models</a></p>
<p><a href="https://theaisummer.com/diffusion-models/"  target="_blank">Sergios Karagiannakos on Diffusion Models</a></p>
<p><a href="https://huggingface.co/blog/annotated-diffusion"  target="_blank">Hugging Face on Annotated Diffusion Model</a></p>
<p><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/"  target="_blank">Assembly AI on Diffusion</a></p>
<p><a href="https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46"  target="_blank">J. Rafid Siddiqui on Latent Diffusion</a></p>
<p><a href="https://www.youtube.com/watch?v=J87hffSMB60"  target="_blank">How does Stable Diffusion work? – Latent Diffusion Models EXPLAINED</a></p>
<p><a href="https://www.youtube.com/watch?v=_7rMfsA24Ls&amp;ab_channel=JeremyHoward"  target="_blank">Stable Diffusion videos from fast.ai</a></p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_posts/2023_01_11_latent_diffusion_models/2023_01_11_latent_diffusion_models.md"
          data-oid-likes="likes_posts/2023_01_11_latent_diffusion_models/2023_01_11_latent_diffusion_models.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/posts/hands-on-latent-diffusion-models/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;Hands on with Latent Diffusion Models
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="2023-01-13T09:28:09&#43;00:00">January 13, 2023</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/posts/marp/">
              <span class="leading-6">
                Creating slides with MARP&ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="2023-01-31T09:06:20&#43;01:00">January 31, 2023</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        


  






<div
  id="scroll-to-top"
  class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2026
          Patrick Schnaß
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
    <script>
    function initTabs() {
        const tabClickHandler = (event) => {
            const button = event.target.closest(".tab__button");
            if (!button) return;

            const container = button.closest(".tab__container");
            if (!container) return;

            const tabIndex = parseInt(button.dataset.tabIndex);
            activateTab(container, tabIndex);
        };

        document.addEventListener("click", tabClickHandler);
    }

    function activateTab(container, activeIndex) {
        const buttons = container.querySelectorAll(".tab__button");
        const panels = container.querySelectorAll(".tab__panel");

        buttons.forEach((btn, index) => {
            if (index === activeIndex) {
                btn.classList.add("tab--active");
                btn.setAttribute("aria-selected", "true");
                
                btn.style.borderColor = "var(--color-primary-500)";
                btn.style.color = "var(--color-primary-500)";
            } else {
                btn.classList.remove("tab--active");
                btn.setAttribute("aria-selected", "false");
                btn.style.borderColor = "transparent";
                btn.style.color = "inherit";
            }
        });

        panels.forEach((panel, index) => {
            if (index === activeIndex) {
                panel.classList.remove("hidden");
                panel.classList.add("block");
            } else {
                panel.classList.add("hidden");
                panel.classList.remove("block");
            }
        });
    }

    
    if (document.readyState === "loading") {
        document.addEventListener("DOMContentLoaded", initTabs);
    } else {
        initTabs();
    }
</script>
  
</footer>

    </div>
  </body>
  
</html>
